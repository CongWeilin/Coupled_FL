{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from synthetic_dataloader import train_val_dataloader\n",
    "from gd import LrdGD, GD\n",
    "from Synthetic import SyntheticDataset\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config['local_batch_size'] = 10\n",
    "config['local_iters'] = 20           # E\n",
    "config['global_iters'] = 200         # T\n",
    "\n",
    "config['num_devices'] = 30           # p\n",
    "config['num_active_devices'] = 10\n",
    "\n",
    "config['lr'] = 0.01\n",
    "config['num_classes'] = 10\n",
    "config['device'] = 'cuda:0'\n",
    "\n",
    "config['iid'] = 0\n",
    "\n",
    "config['alpha'] = 0\n",
    "config['beta'] = 0\n",
    "\n",
    "config['dimension'] = 60\n",
    "\n",
    "config_path = '../config/synthetic_iid.yaml' \n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0, 'beta': 0, 'device': 'cuda:0', 'dimension': 60, 'global_iters': 200, 'iid': 0, 'local_batch_size': 10, 'local_iters': 20, 'lr': 0.01, 'num_active_devices': 10, 'num_classes': 10, 'num_devices': 30}\n"
     ]
    }
   ],
   "source": [
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_hidden = nn.Linear(dim_in, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_hidden(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "device = torch.device(config['device'])\n",
    "    \n",
    "global_model = MLP(dim_in=config['dimension'], dim_out=config['num_classes']).to(device)\n",
    "# global_optim = torch.optim.SGD(global_model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
    "# global_optim = LrdGD(global_model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
    "# global_optim = GD(global_model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
    "\n",
    "local_model_list = []\n",
    "local_optim_list = []\n",
    "for local_id in range(config['num_devices']):\n",
    "    local_model = MLP(dim_in=config['dimension'], dim_out=config['num_classes']).to(device)\n",
    "    local_optim = GD(local_model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
    "    # local_optim = torch.optim.SGD(local_model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
    "    # local_optim = LrdGD(local_model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
    "    local_model_list.append(local_model)\n",
    "    local_optim_list.append(local_optim)\n",
    "\n",
    "global_init_weight = copy.deepcopy(global_model.state_dict())\n",
    "local_model_init_weight_list = []\n",
    "for local_id in range(config['num_devices']):\n",
    "    local_weight = copy.deepcopy(local_model_list[local_id].state_dict())\n",
    "    local_model_init_weight_list.append(local_weight)\n",
    "\n",
    "criterion = nn.NLLLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, dataloader):\n",
    "    \"\"\" Returns the inference accuracy and loss.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    total, correct = 0.0, 0.0\n",
    "    loss = list()\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Inference\n",
    "        outputs = model(images)\n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        loss += [batch_loss.item()]\n",
    "\n",
    "        # Prediction\n",
    "        _, pred_labels = torch.max(outputs, 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "\n",
    "    accuracy = correct/total\n",
    "    loss = sum(loss)/len(loss)\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_state_dicts(w, weight):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights or gradients.\n",
    "    \"\"\"\n",
    "    weight = weight/sum(weight)\n",
    "    \n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        w_avg[key] = torch.zeros_like(w_avg[key])\n",
    "        for i in range(len(w)):\n",
    "            w_avg[key] = w_avg[key] + w[i][key]*weight[i]\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global 1, acc 0.104004, loss 2.348573\n",
      "global 2, acc 0.196250, loss 2.272546\n",
      "global 3, acc 0.284714, loss 2.235676\n",
      "global 4, acc 0.346671, loss 2.204842\n",
      "global 5, acc 0.400072, loss 2.178353\n",
      "global 6, acc 0.446480, loss 2.157291\n",
      "global 7, acc 0.455227, loss 2.140608\n",
      "global 8, acc 0.461343, loss 2.122640\n",
      "global 9, acc 0.481048, loss 2.110097\n",
      "global 10, acc 0.503477, loss 2.098779\n",
      "global 11, acc 0.519802, loss 2.087358\n",
      "global 12, acc 0.537644, loss 2.077344\n",
      "global 13, acc 0.543943, loss 2.070010\n",
      "global 14, acc 0.552603, loss 2.060399\n",
      "global 15, acc 0.557559, loss 2.052804\n",
      "global 16, acc 0.568651, loss 2.047094\n",
      "global 17, acc 0.583714, loss 2.042224\n",
      "global 18, acc 0.586457, loss 2.036509\n",
      "global 19, acc 0.597384, loss 2.032245\n",
      "global 20, acc 0.601012, loss 2.027309\n",
      "global 21, acc 0.603734, loss 2.022967\n",
      "global 22, acc 0.607307, loss 2.016768\n",
      "global 23, acc 0.618585, loss 2.012919\n",
      "global 24, acc 0.628164, loss 2.010095\n",
      "global 25, acc 0.633407, loss 2.007187\n",
      "global 26, acc 0.641044, loss 2.003490\n",
      "global 27, acc 0.646290, loss 2.000199\n",
      "global 28, acc 0.645520, loss 1.997062\n",
      "global 29, acc 0.651509, loss 1.994839\n",
      "global 30, acc 0.652269, loss 1.991720\n",
      "global 31, acc 0.651029, loss 1.989117\n",
      "global 32, acc 0.648991, loss 1.986624\n",
      "global 33, acc 0.648963, loss 1.984131\n",
      "global 34, acc 0.652107, loss 1.981520\n",
      "global 35, acc 0.653367, loss 1.979417\n",
      "global 36, acc 0.653601, loss 1.977130\n",
      "global 37, acc 0.656446, loss 1.975082\n",
      "global 38, acc 0.656929, loss 1.973160\n",
      "global 39, acc 0.655259, loss 1.971160\n",
      "global 40, acc 0.655534, loss 1.969205\n",
      "global 41, acc 0.659560, loss 1.967456\n",
      "global 42, acc 0.661671, loss 1.965756\n",
      "global 43, acc 0.663393, loss 1.963618\n",
      "global 44, acc 0.662530, loss 1.961939\n",
      "global 45, acc 0.661864, loss 1.960364\n",
      "global 46, acc 0.668171, loss 1.958502\n",
      "global 47, acc 0.665197, loss 1.957014\n",
      "global 48, acc 0.668184, loss 1.955258\n",
      "global 49, acc 0.672020, loss 1.953656\n",
      "global 50, acc 0.670619, loss 1.952169\n",
      "global 51, acc 0.673798, loss 1.950676\n",
      "global 52, acc 0.673981, loss 1.948906\n",
      "global 53, acc 0.675789, loss 1.947734\n",
      "global 54, acc 0.675122, loss 1.946287\n",
      "global 55, acc 0.675776, loss 1.944803\n",
      "global 56, acc 0.678886, loss 1.943450\n",
      "global 57, acc 0.680897, loss 1.942281\n",
      "global 58, acc 0.680377, loss 1.940914\n",
      "global 59, acc 0.681215, loss 1.939698\n",
      "global 60, acc 0.681797, loss 1.938513\n",
      "global 61, acc 0.683633, loss 1.937454\n",
      "global 62, acc 0.684675, loss 1.936364\n",
      "global 63, acc 0.685008, loss 1.935147\n",
      "global 64, acc 0.685136, loss 1.933953\n",
      "global 65, acc 0.682912, loss 1.932637\n",
      "global 66, acc 0.685428, loss 1.931438\n",
      "global 67, acc 0.688839, loss 1.930443\n",
      "global 68, acc 0.688814, loss 1.929281\n",
      "global 69, acc 0.687773, loss 1.928245\n",
      "global 70, acc 0.688698, loss 1.927359\n",
      "global 71, acc 0.689568, loss 1.926257\n",
      "global 72, acc 0.689327, loss 1.925239\n",
      "global 73, acc 0.690213, loss 1.924208\n",
      "global 74, acc 0.691942, loss 1.923311\n",
      "global 75, acc 0.691558, loss 1.922302\n",
      "global 76, acc 0.691704, loss 1.921186\n",
      "global 77, acc 0.692600, loss 1.920375\n",
      "global 78, acc 0.692584, loss 1.919539\n",
      "global 79, acc 0.693775, loss 1.918679\n",
      "global 80, acc 0.693391, loss 1.917820\n",
      "global 81, acc 0.694045, loss 1.916937\n",
      "global 82, acc 0.695865, loss 1.916185\n",
      "global 83, acc 0.695331, loss 1.915272\n",
      "global 84, acc 0.695331, loss 1.914458\n",
      "global 85, acc 0.695985, loss 1.913535\n",
      "global 86, acc 0.696638, loss 1.912771\n",
      "global 87, acc 0.697292, loss 1.912016\n",
      "global 88, acc 0.696638, loss 1.911249\n",
      "global 89, acc 0.697292, loss 1.910369\n",
      "global 90, acc 0.696663, loss 1.909601\n",
      "global 91, acc 0.697292, loss 1.908869\n",
      "global 92, acc 0.697821, loss 1.908097\n",
      "global 93, acc 0.698932, loss 1.907351\n",
      "global 94, acc 0.698932, loss 1.906656\n",
      "global 95, acc 0.698599, loss 1.905901\n",
      "global 96, acc 0.698070, loss 1.905137\n",
      "global 97, acc 0.697959, loss 1.904467\n",
      "global 98, acc 0.697959, loss 1.903725\n",
      "global 99, acc 0.699946, loss 1.903143\n",
      "global 100, acc 0.701612, loss 1.902394\n",
      "global 101, acc 0.701612, loss 1.901763\n",
      "global 102, acc 0.702390, loss 1.901049\n",
      "global 103, acc 0.701208, loss 1.900250\n",
      "global 104, acc 0.700579, loss 1.899621\n",
      "global 105, acc 0.700912, loss 1.898982\n",
      "global 106, acc 0.700258, loss 1.898349\n",
      "global 107, acc 0.700579, loss 1.897782\n",
      "global 108, acc 0.699925, loss 1.897115\n",
      "global 109, acc 0.698801, loss 1.896408\n",
      "global 110, acc 0.700163, loss 1.895809\n",
      "global 111, acc 0.700300, loss 1.895193\n",
      "global 112, acc 0.699779, loss 1.894641\n",
      "global 113, acc 0.700655, loss 1.894047\n",
      "global 114, acc 0.699434, loss 1.893436\n",
      "global 115, acc 0.701634, loss 1.892849\n",
      "global 116, acc 0.701104, loss 1.892226\n",
      "global 117, acc 0.700438, loss 1.891677\n",
      "global 118, acc 0.698177, loss 1.891035\n",
      "global 119, acc 0.698806, loss 1.890483\n",
      "global 120, acc 0.700114, loss 1.889937\n",
      "global 121, acc 0.702100, loss 1.889370\n",
      "global 122, acc 0.702100, loss 1.888855\n",
      "global 123, acc 0.700913, loss 1.888281\n",
      "global 124, acc 0.701759, loss 1.887682\n",
      "global 125, acc 0.702272, loss 1.887108\n",
      "global 126, acc 0.701106, loss 1.886649\n",
      "global 127, acc 0.701106, loss 1.886015\n",
      "global 128, acc 0.700585, loss 1.885456\n",
      "global 129, acc 0.699731, loss 1.884986\n",
      "global 130, acc 0.699731, loss 1.884458\n",
      "global 131, acc 0.700026, loss 1.883957\n",
      "global 132, acc 0.699359, loss 1.883427\n",
      "global 133, acc 0.699693, loss 1.882851\n",
      "global 134, acc 0.700359, loss 1.882371\n",
      "global 135, acc 0.699839, loss 1.881781\n",
      "global 136, acc 0.699839, loss 1.881240\n",
      "global 137, acc 0.701172, loss 1.880766\n",
      "global 138, acc 0.702026, loss 1.880309\n",
      "global 139, acc 0.702680, loss 1.879811\n",
      "global 140, acc 0.701876, loss 1.879358\n",
      "global 141, acc 0.701876, loss 1.878964\n",
      "global 142, acc 0.701159, loss 1.878502\n",
      "global 143, acc 0.702492, loss 1.878054\n",
      "global 144, acc 0.703542, loss 1.877665\n",
      "global 145, acc 0.703346, loss 1.877205\n",
      "global 146, acc 0.701051, loss 1.876736\n",
      "global 147, acc 0.702392, loss 1.876240\n",
      "global 148, acc 0.702392, loss 1.875848\n",
      "global 149, acc 0.703908, loss 1.875388\n",
      "global 150, acc 0.704438, loss 1.874924\n",
      "global 151, acc 0.705587, loss 1.874471\n",
      "global 152, acc 0.704254, loss 1.874015\n",
      "global 153, acc 0.704783, loss 1.873644\n",
      "global 154, acc 0.703733, loss 1.873191\n",
      "global 155, acc 0.704262, loss 1.872696\n",
      "global 156, acc 0.705596, loss 1.872271\n",
      "global 157, acc 0.707424, loss 1.871838\n",
      "global 158, acc 0.706877, loss 1.871481\n",
      "global 159, acc 0.708210, loss 1.871106\n",
      "global 160, acc 0.708731, loss 1.870635\n",
      "global 161, acc 0.709384, loss 1.870209\n",
      "global 162, acc 0.706903, loss 1.869785\n",
      "global 163, acc 0.706927, loss 1.869382\n",
      "global 164, acc 0.708115, loss 1.868973\n",
      "global 165, acc 0.709568, loss 1.868602\n",
      "global 166, acc 0.710235, loss 1.868263\n",
      "global 167, acc 0.710235, loss 1.867812\n",
      "global 168, acc 0.709706, loss 1.867380\n",
      "global 169, acc 0.709706, loss 1.866991\n",
      "global 170, acc 0.711459, loss 1.866644\n",
      "global 171, acc 0.710855, loss 1.866234\n",
      "global 172, acc 0.710955, loss 1.865853\n",
      "global 173, acc 0.710434, loss 1.865481\n",
      "global 174, acc 0.710955, loss 1.865079\n",
      "global 175, acc 0.711559, loss 1.864735\n",
      "global 176, acc 0.711559, loss 1.864361\n",
      "global 177, acc 0.710434, loss 1.863983\n",
      "global 178, acc 0.711030, loss 1.863581\n",
      "global 179, acc 0.711030, loss 1.863183\n",
      "global 180, acc 0.711559, loss 1.862858\n",
      "global 181, acc 0.711683, loss 1.862455\n",
      "global 182, acc 0.714273, loss 1.862116\n",
      "global 183, acc 0.714406, loss 1.861714\n",
      "global 184, acc 0.714430, loss 1.861340\n",
      "global 185, acc 0.713972, loss 1.861028\n",
      "global 186, acc 0.713777, loss 1.860647\n",
      "global 187, acc 0.714538, loss 1.860235\n",
      "global 188, acc 0.714538, loss 1.859895\n",
      "global 189, acc 0.713248, loss 1.859489\n",
      "global 190, acc 0.714430, loss 1.859174\n",
      "global 191, acc 0.713901, loss 1.858826\n",
      "global 192, acc 0.714422, loss 1.858456\n",
      "global 193, acc 0.714422, loss 1.858126\n",
      "global 194, acc 0.714422, loss 1.857752\n",
      "global 195, acc 0.715051, loss 1.857455\n",
      "global 196, acc 0.713768, loss 1.857107\n",
      "global 197, acc 0.714397, loss 1.856763\n",
      "global 198, acc 0.715051, loss 1.856427\n",
      "global 199, acc 0.714827, loss 1.856153\n",
      "global 200, acc 0.714297, loss 1.855780\n",
      "global 201, acc 0.714397, loss 1.855449\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "generate training data\n",
    "\"\"\"\n",
    "# X_split, y_split, weight_per_user = generate_synthetic(alpha=config['alpha'], beta=config['beta'], iid=config['iid'], \n",
    "#                                       num_user=config['num_devices'], dimension=config['dimension'],\n",
    "#                                       num_class=config['num_classes'])\n",
    "\n",
    "# trainloader_list, validloader_list = [], []\n",
    "# trainloader_iterator_list, validloader_iterator_list = [], []\n",
    "# for local_id in range(config['num_devices']):\n",
    "#     trainloader, validloader = train_val_dataloader(X_split, y_split, local_id, batch_size=config['local_batch_size'])\n",
    "#     trainloader_list.append(trainloader)\n",
    "#     validloader_list.append(validloader)\n",
    "    \n",
    "#     trainloader_iterator_list.append(iter(trainloader_list[local_id]))\n",
    "#     validloader_iterator_list.append(iter(validloader_list[local_id]))\n",
    "\n",
    "synthetic_dataset = SyntheticDataset(num_classes=config['num_classes'], \n",
    "                                     num_tasks=config['num_devices'], \n",
    "                                     num_dim=config['dimension'],\n",
    "                                     alpha=config['alpha'], beta=config['beta'])\n",
    "data = synthetic_dataset.get_all_tasks()\n",
    "num_samples = synthetic_dataset.get_num_samples()\n",
    "weight_per_user = num_samples/sum(num_samples)\n",
    "\n",
    "trainloader_list, validloader_list = [], []\n",
    "trainloader_iterator_list, validloader_iterator_list = [], []\n",
    "for local_id in range(config['num_devices']):\n",
    "    trainloader, validloader = train_val_dataloader(data[local_id]['x'], data[local_id]['y'], batch_size=config['local_batch_size'])\n",
    "    trainloader_list.append(trainloader)\n",
    "    validloader_list.append(validloader)\n",
    "    \n",
    "    trainloader_iterator_list.append(iter(trainloader_list[local_id]))\n",
    "    validloader_iterator_list.append(iter(validloader_list[local_id]))\n",
    "# for local_id in range(config['num_devices']):\n",
    "#     print(data[local_id]['x'].shape, data[local_id]['y'].shape, num_samples[local_id])\n",
    "    \n",
    "\"\"\"\n",
    "load initial value\n",
    "\"\"\"\n",
    "global_model.load_state_dict(global_init_weight)\n",
    "for local_id in range(config['num_devices']):\n",
    "    local_model_list[local_id].load_state_dict(local_model_init_weight_list[local_id])\n",
    "\n",
    "\"\"\"\n",
    "start training\n",
    "\"\"\"\n",
    "global_acc = []\n",
    "global_loss = []\n",
    "    \n",
    "# test global model\n",
    "list_acc, list_loss = [], [] \n",
    "for local_id in range(config['num_devices']):\n",
    "    acc, loss = inference(global_model, validloader_list[local_id])\n",
    "    list_acc.append(acc)\n",
    "    list_loss.append(loss)\n",
    "global_acc +=  [sum(list_acc)/len(list_acc)]\n",
    "global_loss += [sum(list_loss)/len(list_loss)]\n",
    "print('global %d, acc %f, loss %f'%(len(global_acc), global_acc[-1], global_loss[-1]))\n",
    "    \n",
    "for global_iter in range(config['global_iters']): # T\n",
    "    activate_devices = np.random.permutation(config['num_devices'])[:config['num_active_devices']] # np.arange(config['num_devices'])\n",
    "    # get the local grad for each device\n",
    "    for local_id in activate_devices: # K\n",
    "        for local_iter in range(config['local_iters']): # E\n",
    "            # load single mini-batch\n",
    "            try:\n",
    "                inputs, labels = next(trainloader_iterator_list[local_id])\n",
    "            except StopIteration:\n",
    "                trainloader_iterator_list[local_id] = iter(trainloader_list[local_id])\n",
    "                inputs, labels = next(trainloader_iterator_list[local_id])\n",
    "\n",
    "            # train local model\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            local_model_list[local_id].train()\n",
    "            local_model_list[local_id].zero_grad()\n",
    "            log_probs = local_model_list[local_id](inputs)\n",
    "            loss = criterion(log_probs, labels)\n",
    "            loss.backward()\n",
    "            local_optim_list[local_id].step() # lr=config['lr']/(1+global_iter)\n",
    "            \n",
    "        local_optim_list[local_id].inverse_prop_decay_learning_rate(global_iter)\n",
    "        # acc, loss = inference(local_model_list[local_id], validloader_list[local_id])\n",
    "        # print('local id %d, acc %f, loss %f'%(local_id, acc, loss))\n",
    "    \n",
    "    # average local models \n",
    "    local_weight_list = [local_model.state_dict() for local_model in local_model_list]\n",
    "    avg_local_weight = average_state_dicts(local_weight_list, weight_per_user)\n",
    "    global_model.load_state_dict(avg_local_weight)\n",
    "    for local_id in range(config['num_devices']):\n",
    "        local_model_list[local_id].load_state_dict(avg_local_weight)\n",
    "        \n",
    "    # test global model\n",
    "    list_acc, list_loss = [], [] \n",
    "    for local_id in range(config['num_devices']):\n",
    "        acc, loss = inference(global_model, validloader_list[local_id])\n",
    "        list_acc.append(acc)\n",
    "        list_loss.append(loss)\n",
    "    global_acc +=  [sum(list_acc)/len(list_acc)]\n",
    "    global_loss += [sum(list_loss)/len(list_loss)]\n",
    "    print('global %d, acc %f, loss %f'%(len(global_acc), global_acc[-1], global_loss[-1]))\n",
    "    \n",
    "with open('fedavg_%.2f.pkl'%config['beta'], 'wb') as f:\n",
    "    pickle.dump([global_acc, global_loss], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "\n",
    "# diversity 0\n",
    "with open('fedavg_0.00.pkl', 'rb') as f:\n",
    "    global_acc, global_loss = pickle.load(f)\n",
    "    \n",
    "y = global_acc\n",
    "x = np.arange(len(y))\n",
    "axs.plot(x,y,label='Diversity 0')\n",
    "\n",
    "# # diversity 0.25\n",
    "# with open('fedavg_0.25.pkl', 'rb') as f:\n",
    "#     global_acc, global_loss = pickle.load(f)\n",
    "    \n",
    "# y = global_acc\n",
    "# x = np.arange(len(y))\n",
    "# axs.plot(x,y,label='Diversity 0.25')\n",
    "\n",
    "# diversity 0.5\n",
    "with open('fedavg_0.50.pkl', 'rb') as f:\n",
    "    global_acc, global_loss = pickle.load(f)\n",
    "    \n",
    "y = global_acc\n",
    "x = np.arange(len(y))\n",
    "axs.plot(x,y,label='Diversity 0.5')\n",
    "\n",
    "# # diversity 0.75\n",
    "# with open('fedavg_0.75.pkl', 'rb') as f:\n",
    "#     global_acc, global_loss = pickle.load(f)\n",
    "    \n",
    "# y = global_acc\n",
    "# x = np.arange(len(y))\n",
    "# axs.plot(x,y,label='Diversity 0.75')\n",
    "\n",
    "# diversity 1\n",
    "with open('fedavg_1.00.pkl', 'rb') as f:\n",
    "    global_acc, global_loss = pickle.load(f)\n",
    "    \n",
    "y = global_acc\n",
    "x = np.arange(len(y))\n",
    "axs.plot(x,y,label='Diversity 1')\n",
    "    \n",
    "axs.set_xlabel('Communication round')\n",
    "axs.set_ylabel('Global model accuracy')\n",
    "axs.grid(True)\n",
    "\n",
    "plt.title('Synthetic dataset - Accuracy / Communication round')\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('fedavg_diff_diversity.pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0,\n",
       " 'beta': 0,\n",
       " 'device': 'cuda:0',\n",
       " 'dimension': 60,\n",
       " 'global_iters': 200,\n",
       " 'iid': 0,\n",
       " 'local_batch_size': 10,\n",
       " 'local_iters': 20,\n",
       " 'lr': 0.01,\n",
       " 'num_active_devices': 10,\n",
       " 'num_classes': 10,\n",
       " 'num_devices': 30}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
